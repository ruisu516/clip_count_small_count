{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/susu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/susu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/susu/.netrc\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chunk import RegexpParser\n",
    "import wandb\n",
    "import requests\n",
    "import json\n",
    "from io import BytesIO\n",
    "import svgling\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# Initialize NLTK\n",
    "\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wandb.login(key = \"f120e5e4c8c84329e87f496f85e6f7ded7732680\")\n",
    "\n",
    "concrete_lexnames = {\n",
    "    'noun.animal', 'noun.artifact', 'noun.body', 'noun.food', 'noun.group',\n",
    "    'noun.location', 'noun.object', 'noun.person', 'noun.plant', 'noun.substance', \n",
    "}\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT>?<JJ>*<NNS|NNPS>+}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_log(url,obj, caption):\n",
    "    try:\n",
    "        img_response = requests.get(url, timeout=10)\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"The request timed out. Skipping this image.\")\n",
    "        return\n",
    "    img = Image.open(BytesIO(img_response.content))\n",
    "        \n",
    "    wandb.init(project=\"flicker_raw_images\", entity=\"ruisu\")\n",
    "    wandb.log({\n",
    "        \"url\": url,\n",
    "        'caption': caption,\n",
    "        'number': \"ten\",\n",
    "        \"image\":wandb.Image(img, caption=caption),\n",
    "        \"object\":obj,\n",
    "        \"dataset_name\":None,\n",
    "        \"org_id\":None\n",
    "    })\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_concrete_noun(word):\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    for synset in synsets:\n",
    "        # print(word, synset.lexname() )\n",
    "        print(word, synset.lexname())\n",
    "        if synset.lexname() in concrete_lexnames:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_concrete_noun_phrase(phrase):\n",
    "    tokens = word_tokenize(phrase)\n",
    "    tagged = pos_tag(tokens)\n",
    "    for token, tag in tagged:\n",
    "        if tag in ['NN', 'NNS', 'NNP', 'NNPS']:  # Check if the token is a noun\n",
    "            if is_concrete_noun(token):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def find_plural_noun_phrases(text,num_word):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    # Create a parser with our grammar\n",
    "    cp = RegexpParser(grammar)\n",
    "    tree = cp.parse(tagged)\n",
    "\n",
    "    results = []\n",
    "    current_position = 0\n",
    "    previous_word = None\n",
    "\n",
    "    # Iterate through the tree to find number words followed by NP chunks\n",
    "    print(tree)\n",
    "    for node in tree:\n",
    "        if isinstance(node, nltk.Tree):\n",
    "            if node.label() == 'NP':\n",
    "                \n",
    "                if previous_word and previous_word == num_word:\n",
    "                    np_text = ' '.join(token for token, pos in node.leaves())\n",
    "                    if is_concrete_noun_phrase(np_text):\n",
    "                        results.append((previous_word, np_text))\n",
    "                elif node.leaves()[0][0] == num_word:\n",
    "                    np_text = ' '.join(token for token, pos in node.leaves()[1:])\n",
    "                    if is_concrete_noun_phrase(np_text):\n",
    "                        results.append((num_word, np_text))\n",
    "            current_position += len(node.leaves())\n",
    "        else:\n",
    "            token, tag = node\n",
    "            previous_word = token\n",
    "            current_position += 1\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_sample(photo,dataset_name,numbers):\n",
    "    caption = photo[\"caption\"]\n",
    "\n",
    "    num_candidates = []\n",
    "    for number in numbers:\n",
    "        if number in caption.split(\" \"):\n",
    "            num_candidates.append(number)\n",
    "    # print(\"len(num_candidates)\",)\n",
    "    if len(num_candidates) == 0:\n",
    "        return \"no number\"\n",
    "\n",
    "    # api = wandb.Api()\n",
    "    # runs = api.runs(path=\"ruisu/flicker_raw_images\")\n",
    "    # exist = False\n",
    "    # for run in runs:\n",
    "    #     if run.config.get(\"url\") == photo[\"url\"]:\n",
    "    #         exist = True\n",
    "    #         print(f\"Already exists: {photo['url']}\")\n",
    "    #         break\n",
    "\n",
    "    # if exist:\n",
    "    #     return \"exist\"\n",
    "\n",
    "    for number in num_candidates:\n",
    "\n",
    "        found_phrases = find_plural_noun_phrases(caption,number)\n",
    "        print(\"found_phrases\",found_phrases)\n",
    "        print(\"caption\",caption)\n",
    "        if len(found_phrases)<= 0:\n",
    "            continue\n",
    "        try:\n",
    "        \n",
    "            try:\n",
    "                # Set a timeout of 10 seconds\n",
    "                img_response = requests.get(photo[\"url\"], timeout=10)\n",
    "            except requests.exceptions.Timeout:\n",
    "                # Handle the timeout exception\n",
    "                print(\"The request timed out. Skipping this image.\")\n",
    "                continue\n",
    "            img = Image.open(BytesIO(img_response.content))\n",
    "            # inputs = processor(text=[caption], images=[img], return_tensors=\"pt\", padding=True)\n",
    "            # outputs = model(**inputs)\n",
    "            # if outputs.logits_per_image.item() < 30:  # threshold can be adjusted\n",
    "            #     continue\n",
    "\n",
    "            wandb.init(project=\"flicker_raw_images\", entity=\"ruisu\")\n",
    "            wandb.log({\n",
    "                \"url\": photo[\"url\"],\n",
    "                'caption': caption,\n",
    "                'number': number,\n",
    "                \"image\":wandb.Image(img, caption=caption),\n",
    "                \"object\":found_phrases[0][1],\n",
    "                \"dataset_name\":dataset_name,\n",
    "                \"org_id\":photo[\"org_id\"]\n",
    "            })\n",
    "            wandb.finish()\n",
    "            return \"sucess\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process image {photo['url']}: {e}\")\n",
    "            continue\n",
    "    return \"wrong grammar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title conceptual_12m\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "dataset = load_dataset(\"conceptual_12m\",split=\"train\",streaming=True)\n",
    "numbers = [\"six\", \"seven\", \"eight\", \"nine\"]\n",
    "# numbers = [\"four\"]\n",
    "# check the lower bound of ten: 6292946\n",
    "pbar = tqdm.tqdm(dataset)\n",
    "for i, sample in enumerate(pbar):\n",
    "    if i <= 7434944: #numbers = [\"six\", \"seven\", \"eight\", \"nine\", \"ten\"]\n",
    "        continue\n",
    "    # if i >= 315243:\n",
    "    #     break\n",
    "    photo = {\n",
    "        \"caption\":sample[\"caption\"].lower(),\n",
    "        \"url\":sample[\"image_url\"],\n",
    "        \"org_id\":i\n",
    "    }\n",
    "    return_code = process_sample(photo,\"conceptual_12m-train\",numbers)\n",
    "\n",
    "    pbar.set_description(f\"Code: {return_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_json_list = [\n",
    "    {\n",
    "        \"number\": 10,\n",
    "        \"target\": \"apples\",\n",
    "        \"target_context\": \"ten apples\",\n",
    "        \"image_url\": \"https://media.baamboozle.com/uploads/images/189889/1636331500_50014_url.png\",\n",
    "        \"dataset_name\": \"\",\n",
    "        \"org_id\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"number\": 10,\n",
    "        \"target\": \"stars\",\n",
    "        \"target_context\": \"ten stars\",\n",
    "        \"image_url\": \"https://vt-vtwa-assets.varsitytutors.com/vt-vtwa/uploads/problem_question_image/image/28099/10.png\",\n",
    "        \"dataset_name\": \"\",\n",
    "        \"org_id\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"number\": 10,\n",
    "        \"target\": \"animals\",\n",
    "        \"target_context\": \"ten cartoon animals\",\n",
    "        \"image_url\": \"https://cdn.vectorstock.com/i/1000x1000/89/10/ten-cute-animals-vector-37188910.webp\",\n",
    "        \"dataset_name\": \"\",\n",
    "        \"org_id\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"number\": 10,\n",
    "        \"target\": \"animals\",\n",
    "        \"target_context\": \"ten cute animals\",\n",
    "        \"image_url\": \"https://img.freepik.com/premium-vector/set-10-cute-wild-animals_41303-28.jpg\",\n",
    "        \"dataset_name\": \"\",\n",
    "        \"org_id\": \"\"\n",
    "    }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipcount",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
